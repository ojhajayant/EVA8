{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### add these features to BERT training:\n",
        "\n",
        "*  collect your own data (cannot be Shakespeare or any single file downloaded from the internet. Your sources should come from multiple URLs (basically copy paste 1000s of times)\n",
        "\n",
        "* noisy word prediction (swap any word 15% of times from a sentence with any other random word, and then predict the correct word):\n",
        "\n",
        "    > Share a sample from your own dataset \n",
        "\n",
        "    > Share the training log (Epochs/x = 10 logs)\n",
        "\n",
        "    > Share 10 examples of input-output"
      ],
      "metadata": {
        "id": "tZHJWCwXPI5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here is the code to get a \"training.txt\" file from few urls"
      ],
      "metadata": {
        "id": "N0c7A-S-Pjj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def extract_text_from_url(url, filename):\n",
        "    # Send a GET request to the URL and retrieve the response\n",
        "    response = requests.get(url)\n",
        "    \n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Decode the response text using the iso-8859-1 encoding\n",
        "        response_text = response.content.decode('utf-8')\n",
        "        \n",
        "        # Open a new text file and append the decoded response text to it\n",
        "        with open(filename, 'a', encoding='utf-8') as file:\n",
        "              file.write(response_text)\n",
        "        print(f\"Text extracted from {url} and saved to {filename} successfully!\")\n",
        "    else:\n",
        "        print(f\"Error {response.status_code}: Could not retrieve text from {url}\")\n",
        "\n",
        "# Extract the URLs for each of Dickens' works\n",
        "base_url = \"https://www.gutenberg.org/files/\"\n",
        "dickens_ids = [\"98\", \"1400\", \"766\", \"580\", \"786\", \"888\", \"963\", \"27924\", \"730\"]\n",
        "austen_ids = [\"1342\", \"158\", \"105\"]\n",
        "twain_ids = [\"76\", \"74\", \"219\"]\n",
        "\n",
        "filename = \"training.txt\"\n",
        "\n",
        "# Loop through the list of Dickens' works and extract the text from each URL\n",
        "for book_id in dickens_ids:\n",
        "    url = base_url + book_id + \"/\" + book_id + \"-0.txt\"\n",
        "    extract_text_from_url(url, filename)\n",
        "\n",
        "# Loop through the list of Jane Austen's works and extract the text from each URL\n",
        "for book_id in austen_ids:\n",
        "    url = base_url + book_id + \"/\" + book_id + \"-0.txt\"\n",
        "    extract_text_from_url(url, filename)\n",
        "\n",
        "# Loop through the list of Mark Twain's works and extract the text from each URL\n",
        "for book_id in twain_ids:\n",
        "    url = base_url + book_id + \"/\" + book_id + \"-0.txt\"\n",
        "    extract_text_from_url(url, filename)\n",
        "\n",
        "print(\"All texts extracted and saved to \" + filename + \" successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n1mIC54bCL6",
        "outputId": "f533218e-d8e4-462c-930c-bea541984b42"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text extracted from https://www.gutenberg.org/files/98/98-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/1400/1400-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/766/766-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/580/580-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/786/786-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/888/888-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/963/963-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/27924/27924-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/730/730-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/1342/1342-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/158/158-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/105/105-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/76/76-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/74/74-0.txt and saved to training.txt successfully!\n",
            "Text extracted from https://www.gutenberg.org/files/219/219-0.txt and saved to training.txt successfully!\n",
            "All texts extracted and saved to training.txt successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model training"
      ],
      "metadata": {
        "id": "GttebaQmP0KQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dqrz-PVAYsJ0",
        "outputId": "82ab6eda-6702-4631-ffc7-2c6b0d0ea476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n",
            "loading text...\n",
            "tokenizing sentences...\n",
            "creating/loading vocab...\n",
            "creating dataset...\n",
            "initializing model...\n",
            "initializing optimizer and loss...\n",
            "training...\n",
            "it: 0  | loss 10.75  | Δw: 3.47\n",
            "it: 10  | loss 9.94  | Δw: 2.825\n",
            "it: 20  | loss 9.46  | Δw: 2.724\n",
            "it: 30  | loss 9.03  | Δw: 2.901\n",
            "it: 40  | loss 8.67  | Δw: 2.844\n",
            "it: 50  | loss 8.34  | Δw: 2.714\n",
            "it: 60  | loss 8.03  | Δw: 2.578\n",
            "it: 70  | loss 7.76  | Δw: 2.488\n",
            "it: 80  | loss 7.53  | Δw: 2.42\n",
            "it: 90  | loss 7.35  | Δw: 2.331\n",
            "it: 100  | loss 7.12  | Δw: 2.279\n",
            "it: 110  | loss 6.86  | Δw: 2.161\n",
            "it: 120  | loss 6.78  | Δw: 2.126\n",
            "it: 130  | loss 6.58  | Δw: 2.068\n",
            "it: 140  | loss 6.43  | Δw: 2.028\n",
            "it: 150  | loss 6.28  | Δw: 1.979\n",
            "it: 160  | loss 6.14  | Δw: 1.932\n",
            "it: 170  | loss 5.98  | Δw: 1.896\n",
            "it: 180  | loss 5.86  | Δw: 1.863\n",
            "it: 190  | loss 5.65  | Δw: 1.814\n",
            "it: 200  | loss 5.56  | Δw: 1.822\n",
            "it: 210  | loss 5.4  | Δw: 1.762\n",
            "it: 220  | loss 5.27  | Δw: 1.751\n",
            "it: 230  | loss 5.21  | Δw: 1.759\n",
            "it: 240  | loss 5.08  | Δw: 1.749\n",
            "it: 250  | loss 4.97  | Δw: 1.72\n",
            "it: 260  | loss 4.83  | Δw: 1.671\n",
            "it: 270  | loss 4.76  | Δw: 1.672\n",
            "it: 280  | loss 4.65  | Δw: 1.693\n",
            "it: 290  | loss 4.61  | Δw: 1.685\n",
            "it: 300  | loss 4.47  | Δw: 1.645\n",
            "it: 310  | loss 4.42  | Δw: 1.659\n",
            "it: 320  | loss 4.33  | Δw: 1.632\n",
            "it: 330  | loss 4.27  | Δw: 1.644\n",
            "it: 340  | loss 4.17  | Δw: 1.607\n",
            "it: 350  | loss 4.17  | Δw: 1.635\n",
            "it: 360  | loss 4.08  | Δw: 1.663\n",
            "it: 370  | loss 3.99  | Δw: 1.62\n",
            "it: 380  | loss 3.93  | Δw: 1.628\n",
            "it: 390  | loss 3.81  | Δw: 1.609\n",
            "it: 400  | loss 3.79  | Δw: 1.576\n",
            "it: 410  | loss 3.78  | Δw: 1.611\n",
            "it: 420  | loss 3.74  | Δw: 1.594\n",
            "it: 430  | loss 3.65  | Δw: 1.57\n",
            "it: 440  | loss 3.56  | Δw: 1.543\n",
            "it: 450  | loss 3.51  | Δw: 1.53\n",
            "it: 460  | loss 3.49  | Δw: 1.574\n",
            "it: 470  | loss 3.46  | Δw: 1.558\n",
            "it: 480  | loss 3.42  | Δw: 1.556\n",
            "it: 490  | loss 3.48  | Δw: 1.561\n",
            "it: 500  | loss 3.37  | Δw: 1.547\n",
            "it: 510  | loss 3.32  | Δw: 1.522\n",
            "it: 520  | loss 3.28  | Δw: 1.506\n",
            "it: 530  | loss 3.25  | Δw: 1.505\n",
            "it: 540  | loss 3.19  | Δw: 1.517\n",
            "it: 550  | loss 3.21  | Δw: 1.49\n",
            "it: 560  | loss 3.17  | Δw: 1.489\n",
            "it: 570  | loss 3.08  | Δw: 1.468\n",
            "it: 580  | loss 3.04  | Δw: 1.479\n",
            "it: 590  | loss 3.09  | Δw: 1.465\n",
            "it: 600  | loss 3.04  | Δw: 1.474\n",
            "it: 610  | loss 2.99  | Δw: 1.458\n",
            "it: 620  | loss 2.94  | Δw: 1.449\n",
            "it: 630  | loss 2.93  | Δw: 1.429\n",
            "it: 640  | loss 2.98  | Δw: 1.418\n",
            "it: 650  | loss 2.9  | Δw: 1.398\n",
            "it: 660  | loss 2.82  | Δw: 1.405\n",
            "it: 670  | loss 2.9  | Δw: 1.414\n",
            "it: 680  | loss 2.88  | Δw: 1.417\n",
            "it: 690  | loss 2.9  | Δw: 1.412\n",
            "it: 700  | loss 2.88  | Δw: 1.416\n",
            "it: 710  | loss 2.81  | Δw: 1.384\n",
            "it: 720  | loss 2.8  | Δw: 1.41\n",
            "it: 730  | loss 2.74  | Δw: 1.377\n",
            "it: 740  | loss 2.69  | Δw: 1.341\n",
            "it: 750  | loss 2.66  | Δw: 1.348\n",
            "it: 760  | loss 2.76  | Δw: 1.416\n",
            "it: 770  | loss 2.61  | Δw: 1.335\n",
            "it: 780  | loss 2.65  | Δw: 1.341\n",
            "it: 790  | loss 2.68  | Δw: 1.352\n",
            "it: 800  | loss 2.65  | Δw: 1.358\n",
            "it: 810  | loss 2.65  | Δw: 1.318\n",
            "it: 820  | loss 2.56  | Δw: 1.33\n",
            "it: 830  | loss 2.65  | Δw: 1.324\n",
            "it: 840  | loss 2.6  | Δw: 1.339\n",
            "it: 850  | loss 2.59  | Δw: 1.323\n",
            "it: 860  | loss 2.58  | Δw: 1.312\n",
            "it: 870  | loss 2.45  | Δw: 1.279\n",
            "it: 880  | loss 2.55  | Δw: 1.294\n",
            "it: 890  | loss 2.4  | Δw: 1.254\n",
            "it: 900  | loss 2.46  | Δw: 1.273\n",
            "it: 910  | loss 2.46  | Δw: 1.232\n",
            "it: 920  | loss 2.45  | Δw: 1.295\n",
            "it: 930  | loss 2.46  | Δw: 1.259\n",
            "it: 940  | loss 2.4  | Δw: 1.202\n",
            "it: 950  | loss 2.43  | Δw: 1.231\n",
            "it: 960  | loss 2.41  | Δw: 1.231\n",
            "it: 970  | loss 2.36  | Δw: 1.195\n",
            "it: 980  | loss 2.51  | Δw: 1.255\n",
            "it: 990  | loss 2.46  | Δw: 1.251\n",
            "saving embeddings...\n",
            "end\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Libs\n",
        "# =============================================================================\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Transformer\n",
        "# =============================================================================\n",
        "def attention(q, k, v, mask = None, dropout = None):\n",
        "    scores = q.matmul(k.transpose(-2, -1))\n",
        "    scores /= math.sqrt(q.shape[-1])\n",
        "    \n",
        "    #mask\n",
        "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
        "    \n",
        "    scores = F.softmax(scores, dim = -1)\n",
        "    scores = dropout(scores) if dropout is not None else scores\n",
        "    output = scores.matmul(v)\n",
        "    return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "#        self.q_linear = nn.Linear(out_dim, out_dim)\n",
        "#        self.k_linear = nn.Linear(out_dim, out_dim)\n",
        "#        self.v_linear = nn.Linear(out_dim, out_dim)\n",
        "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.out_dim = out_dim\n",
        "        self.out_dim_per_head = out_dim // n_heads\n",
        "        self.out = nn.Linear(out_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def split_heads(self, t):\n",
        "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
        "    \n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        #in decoder, y comes from encoder. In encoder, y=x\n",
        "        y = x if y is None else y\n",
        "        \n",
        "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
        "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        \n",
        "        #break into n_heads\n",
        "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
        "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        \n",
        "        #n_heads => attention => merge the heads => mix information\n",
        "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
        "        \n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
        "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #inp => inner => relu => dropout => inner => inp\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x)))) \n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
        "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
        "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        x2 = self.norm1(x)\n",
        "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
        "        x2 = self.norm2(x)\n",
        "        x = x + self.dropout2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        #model input\n",
        "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
        "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
        "        \n",
        "        #backbone\n",
        "        encoders = []\n",
        "        for i in range(n_code):\n",
        "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
        "        self.encoders = nn.ModuleList(encoders)\n",
        "        \n",
        "        #language model\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
        "                \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = x + self.pe(x)\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Positional Embedding\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        pe.requires_grad = False\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len\n",
        "    \n",
        "# =============================================================================\n",
        "# Dataset\n",
        "# =============================================================================\n",
        "class SentencesDataset(Dataset):\n",
        "    #Init dataset\n",
        "    def __init__(self, sentences, vocab, seq_len):\n",
        "        dataset = self\n",
        "        \n",
        "        dataset.sentences = sentences\n",
        "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
        "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)} \n",
        "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
        "        dataset.seq_len = seq_len\n",
        "        \n",
        "        #special tags\n",
        "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
        "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
        "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index, p_mask=0.15, p_noisy=0.15):\n",
        "        dataset = self\n",
        "        \n",
        "        #while we don't have enough word to fill the sentence for a batch\n",
        "        s = []\n",
        "        while len(s) < dataset.seq_len:\n",
        "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
        "            index += 1\n",
        "        \n",
        "        #ensure that the sequence is of length seq_len\n",
        "        s = s[:dataset.seq_len]\n",
        "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
        "        \n",
        "        #apply random mask or noisy word\n",
        "        s = [(dataset.MASK_IDX, w) if random.random() < p_mask else (w, dataset.IGNORE_IDX) if random.random() < p_noisy else (w,w) for w in s]\n",
        "        \n",
        "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
        "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
        "\n",
        "    #return length\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    #get words id\n",
        "    def get_sentence_idx(self, index):\n",
        "        dataset = self\n",
        "        s = dataset.sentences[index]\n",
        "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n",
        "        return s\n",
        "\n",
        "# =============================================================================\n",
        "# Methods / Class\n",
        "# =============================================================================\n",
        "def get_batch(loader, loader_iter):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "    except StopIteration:\n",
        "        loader_iter = iter(loader)\n",
        "        batch = next(loader_iter)\n",
        "    return batch, loader_iter\n",
        "\n",
        "# =============================================================================\n",
        "# #Init\n",
        "# =============================================================================\n",
        "print('initializing..')\n",
        "batch_size = 512\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "# n_workers = 12\n",
        "\n",
        "#optimizer\n",
        "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n",
        "\n",
        "# =============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "#1) load text\n",
        "print('loading text...')\n",
        "pth = 'training.txt'\n",
        "sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "\n",
        "#3) create vocab if not already created\n",
        "print('creating/loading vocab...')\n",
        "pth = 'vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')\n",
        "\n",
        "#4) create dataset\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "#init model\n",
        "print('initializing model...')\n",
        "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()\n",
        "\n",
        "# =============================================================================\n",
        "# Optimizer\n",
        "# =============================================================================\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n",
        "\n",
        "# =============================================================================\n",
        "# Train\n",
        "# =============================================================================\n",
        "print('training...')\n",
        "print_each = 10\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "n_iteration = 1000\n",
        "for it in range(n_iteration):\n",
        "    \n",
        "    #get batch\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
        "    \n",
        "    #infer\n",
        "    masked_input = batch['input']\n",
        "    masked_target = batch['target']\n",
        "    \n",
        "    masked_input = masked_input.cuda(non_blocking=True)\n",
        "    masked_target = masked_target.cuda(non_blocking=True)\n",
        "    output = model(masked_input)\n",
        "    \n",
        "    #compute the cross entropy loss \n",
        "    output_v = output.view(-1,output.shape[-1])\n",
        "    target_v = masked_target.view(-1,1).squeeze()\n",
        "    loss = loss_model(output_v, target_v)\n",
        "    \n",
        "    #compute gradients\n",
        "    loss.backward()\n",
        "    \n",
        "    #apply gradients\n",
        "    optimizer.step()\n",
        "    \n",
        "    #print step\n",
        "    if it % print_each == 0:\n",
        "        print('it:', it, \n",
        "              ' | loss', np.round(loss.item(),2),\n",
        "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
        "    \n",
        "    #reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "\n",
        "# =============================================================================\n",
        "# Results analysis\n",
        "# =============================================================================\n",
        "print('saving embeddings...')\n",
        "N = 3000\n",
        "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
        "\n",
        "\n",
        "print('end')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 examples of input-output"
      ],
      "metadata": {
        "id": "b_poLYUoObxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate 10 input sentences\n",
        "sentences = ['This is the first sentence.',\n",
        "             'Here is the second sentence.',\n",
        "             'The third sentence comes next.',\n",
        "             'We are now on sentence.',\n",
        "             'Sentence number five is here.',\n",
        "             'Let us move sentence six.',\n",
        "             'The seventh sentence up next.',\n",
        "             'Eighth sentence, is right up.',\n",
        "             'The ninth sentence  upon us.',\n",
        "             'This is the final sentence.']\n",
        "\n",
        "# convert the sentences to input tensors\n",
        "inputs = torch.tensor([[dataset.vocab.get(word.lower(), 0)\n",
        "                        for word in sentence.split()] for sentence in sentences])\n",
        "inputs = inputs.to('cuda')\n",
        "# print(\"inputs.shape: \", inputs.shape)\n",
        "\n",
        "# process the inputs through the model\n",
        "outputs = model(inputs)\n",
        "# print(\"outputs.shape: \", outputs.shape)\n",
        "\n",
        "# generate the output sentences\n",
        "output_indices = torch.argmax(outputs, dim=2).tolist()\n",
        "output_sentences = []\n",
        "for indices in output_indices:\n",
        "    words = [dataset.rvocab[index] for index in indices if index != dataset.vocab['<ignore>'] and not isinstance(index, list)]\n",
        "    sentence = ' '.join(words)\n",
        "    output_sentences.append(sentence)\n",
        "\n",
        "# print the output sentences\n",
        "for sentence in output_sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QU_2W9GGt2k",
        "outputId": "4276ed78-73c7-4937-e7ef-9014b375476d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is the first ,\n",
            "here is the second ,\n",
            "the third sentence comes ,\n",
            "we are now on ,\n",
            "sentence number five is ,\n",
            "let us move sentence ,\n",
            "the them sentence up ,\n",
            "observed , is right ,\n",
            "the ‘you sentence upon ,\n",
            "this is the if ,\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}